{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a448b14d-774e-4553-948e-e01c2ad8331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-таблица:\n",
      "[[19.85555793  0.        ]\n",
      " [19.85738353  0.        ]\n",
      " [19.85918607 -0.96      ]\n",
      " [19.86096582  0.7954432 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# правила игры\n",
    "class SimpleEnv:\n",
    "    \n",
    "    # Определяем размер пространства состояний и действий  \n",
    "    def __init__(self):\n",
    "        self.state_space_size = 4  # Например, 4 различных состояния\n",
    "        self.action_space_size = 2  # Например, 2 действия (0 и 1)\n",
    "    \n",
    "    # логикa изменения состояния и назначения награды\n",
    "    def step(self, state, action):\n",
    "        next_state = (state + 1) % self.state_space_size # следующий шаг - предыдущий до начального\n",
    "        reward = 1 if action == 0 else -1 # Награда зависит от выбранного действия\n",
    "        done = next_state == 0 # если вернулись в начальное положение\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    # начальное состояние среды\n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    # Гибридная политика: используем Q-таблицу, если есть положительные значения, иначе случайный выбор\n",
    "    def hybrid_policy(q_table, state):\n",
    "        if np.max(q_table[state]) > 0:\n",
    "            return np.argmax(q_table[state])# *random.choice([0, 1])\n",
    "        else:\n",
    "            return random.choice([0, 1])\n",
    "\n",
    "def train(env, episodes=100, learning_rate=0.8, gamma=0.95):\n",
    "    # Инициализация Q-таблицы нулями\n",
    "    q_table = np.zeros((env.state_space_size, env.action_space_size))\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = hybrid_policy(q_table, state)  # Выбор действия согласно гибридной политике\n",
    "            next_state, reward, done = env.step(state, action)\n",
    "\n",
    "            # Обновление Q-таблицы\n",
    "            q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
    "                                     learning_rate * (reward + gamma * np.max(q_table[next_state]))\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "# Создаем экземпляр среды и обучаем агента\n",
    "env = SimpleEnv()\n",
    "env.reset\n",
    "q_table = train(env)\n",
    "\n",
    "# Выводим обученную Q-таблицу\n",
    "print(\"Q-таблица:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8374fd90-cf84-4928-9e13-a5e1d9f95433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'learning_rate': 0.7000000000000001, 'gamma': 0.4, 'episodes': 10000}\n",
      "Обученная Q-таблица с лучшими параметрами:\n",
      "[[ 1.66666667 -0.7       ]\n",
      " [ 1.66666667  0.        ]\n",
      " [ 1.66666667  0.        ]\n",
      " [ 1.66666667 -0.65912   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningWrapper(BaseEstimator):\n",
    "    def __init__(self, env, episodes=100, learning_rate=0.8, gamma=0.95):\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        q_table = np.zeros((self.env.state_space_size, self.env.action_space_size))\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = hybrid_policy(q_table, state)  # Выбор действия согласно гибридной политике\n",
    "                next_state, reward, done = self.env.step(state, action)\n",
    "\n",
    "                # Обновление Q-таблицы\n",
    "                q_table[state, action] = (1 - self.learning_rate) * q_table[state, action] + \\\n",
    "                                         self.learning_rate * (reward + self.gamma * np.max(q_table[next_state]))\n",
    "                state = next_state\n",
    "\n",
    "        self.q_table_ = q_table\n",
    "        return self\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        return 0\n",
    "\n",
    "# Создаем экземпляр среды\n",
    "env = SimpleEnv()\n",
    "env.reset\n",
    "\n",
    "# Определяем диапазоны для подбора гиперпараметров\n",
    "param_dist = {\n",
    "    'episodes': [1, 10000],\n",
    "    'learning_rate': np.arange(0.1, 1.0, 0.1),\n",
    "    'gamma': np.arange(0.1, 1.0, 0.1)\n",
    "}\n",
    "\n",
    "# Создаем пустые массивы для данных X и y\n",
    "X_dummy = np.empty((10, env.state_space_size))\n",
    "y_dummy = np.empty(10)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=QLearningWrapper(env), param_distributions=param_dist, cv=3, n_iter=10)\n",
    "\n",
    "random_search.fit(X=X_dummy, y=y_dummy)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "\n",
    "# Получаем обученную модель с лучшими параметрами\n",
    "best_q_table = random_search.best_estimator_\n",
    "\n",
    "# Выводим обученную Q-таблицу\n",
    "print(\"Обученная Q-таблица с лучшими параметрами:\")\n",
    "print(best_q_table.q_table_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
